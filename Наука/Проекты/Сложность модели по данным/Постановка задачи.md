**Дано**
1. Обучающее множество $T = \{(x_i, y_i), i = 1\dots d\}$, где $x_i \in R^n$, $y_i \in Dom$
2. При этом это может быть как задача классификации, т.е. $Dom \subset N$, так и регрессия, т.е. $Dom \subseteq R$. 
3. Класс решающих функций $H = \{h_j : R^n \rightarrow Dom\}$ и алгоритм обучения, т.е. $A : T \rightarrow H$

**Найти**
1. Оценку сложности решающей функции $C : T, H \rightarrow R$. То есть такую функцию, которая по обучающей выборке ставит в соответствие решающей функции число.
2. Оценка должна быть монотонно возрастающей, т.е. чем сложнее решающая функция, тем выше её значение.
3. При этом, если $C(T,h) < 0$, то решающая функция слишком проста для задачи, если $C(T,h) > 0$ - сложна, возможно есть эффект переобучения. Оптимальное значение - ноль. 
4. Оценка должна быть дифференцируемой чтобы её можно было использовать как регуляризатор при обучении в том числе и методов, использующих градиентный спуск и его вариации.
5. Оценка должна учитывать сложность исходных данных, то есть она относительна и самого обучающего множества. Если обучающее множество предполагает под собой решающую функцию "константа", то полином десятой степени для нее должен быть сложнее согласно оценке, чем тот же полином для задачи, в которой предполагается полином сотой степени. 

**Цель**
1. Получение регуляризатора, который в сочетании с методами упрощения структуры модели #ml/group_loss позволил бы автоматически подбирать архитектуру решающей функции под конкретную задачу машинного обучения. То есть:
	1. Минимизация переобучения - относительная оценка сложности модели должна позволить понять что модель "нашла" закономерности, которых нет в данных в процессе обучения, а не путем анализа ошибки на валидационной выборке.
	2. Штраф за недообучение - относительная оценка сложности должна подстегивать к усложнению архитектуры решающей функции.