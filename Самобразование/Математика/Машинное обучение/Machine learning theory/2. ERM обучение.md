#ml/erm

---

* Особенность метрики $\mathbb{L}_{\mathbb{D}, f}(h)$ #ml/general_loss в том, что она опирается на неизвестное распределение $\mathbb{D}: X \times Y \rightarrow [0, 1]$. Все что доступно - это выборка $S = \{(x, y): x \sim \mathbb{D},\ y = f(x)\}$ или, в случае если предположение о существовании $f: X \rightarrow Y$ не используется, то $S \subset (X \times Y) \sim \mathbb{D}$. 
* Здесь и далее мы будем опираться на существование функции $f$  и, более того, мы предполагаем важное допущение #ml/terms/reliability_assumption:
$$f \in H$$
	то есть то, что данная функция лежит в пространстве гипотез из которых выбирается гипотеза для классификации (регрессии).
* Таким образом, мы можем определить только *эмпирический риск* (*empirical risk* #ml/tems/empirical_risk):
$$ \mathbb{L}_{S}(h) = \frac{|\{x : x \in S, h(x) \neq f(x)\}|}{|S|} $$
* Алгоритм обучения, который опирается на минимизацию эмпирического риска называется ERM алгоритмом (*empirical risk minimization*):
$$h_S = \min\limits_{h \in H} \mathbb{L}_{S}(h)$$
* Если не ограничивать $H$ никак, то при $|H| = \infty$ алгоритм машинного обучения всегда будет переобучаться, то есть всегда будет возможность найти такое $h$, что $\mathbb{L_{S}(h)} = 0$ и при этом $\mathbb{L_{\mathbb{D},d}}(h) > 0$. Простой пример:
$$h(x) = \begin{cases}
	1,\ if\ x \in S \\
	0,\ otherwise
\end{cases}$$
* Однако есть теорема, которая говорит, что если #ml/terms/reliability_assumption соблюдается и $|H| < \infty$ [^1], то существует алгоритм $ERM_H$, который сходится к решению с любой наперед заданной точностью в $\mathbb{L}_{\mathbb{D},f}(h)$

--

**Теорема:** #ml/erm_h/theorem
> Если:
>	а) зафиксировано #ml/terms/reliability_assumption , то есть существует в пространстве гипотез функция $f$, которая всегда дает верный ответ ($\mathbb{L}_{\mathbb{D},f}$(f) = 0);
>	б) пространство гипотез конечно $|H| < \infty$;
>	в) строки в выборке $S$ представляют собой совокупность независимых друг от друга и одинаково распределенных случайных векторов ( #ml/terms/iid ) и размерность выборки $|S| = m$,
>
>То:
>	$\mathbb{P}\left[ \mathbb{L}_{\mathbb{D},f}(h_{S}) > \epsilon \right] \leq |H|e^{-\epsilon m}$

*Доказательство:*
1. Зафиксируем вероятность того, что на выборке из $m$ элементов $ERM_H$ вернет гипотезу с ошибкой больше $\epsilon$:
$$ \mathbb{P}\left[ \mathbb{L}_{\mathbb{D},f}(h_{S}) > \epsilon \right] $$
Наша задача - ограничить эту вероятность.
2. Зафиксируем множество всех "плохих гипотез", то есть таких гипотез, которые минимизируют эмпирический риск, но ошибаются в генеральной совокупности:
$$ H_B = \{h \in H: \mathbb{L}_{S_B}(h) = 0, \mathbb{L}_{\mathbb{D},f}(h_{S}) > \epsilon\} $$
3. Также зафиксируем множество всех таких обучающих множеств, что обучение на них приводит к плохой гипотезе:
$$ M = \{S_B : \exists h \in H_B, \mathbb{L}_{S_B}(h) = 0\} $$
4. Исходя из #ml/terms/reliability_assumption и из того, что $ERM_H$ *вернет либо плохую модель, либо* $h^{*} = f$, следует, что если попалась какая-то модель, которая ошибается, то это возможно тогда и только тогда, когда нам попалось "плохое обучающее множество", т.е.
$$ \{S : \mathbb{L}_{\mathbb{D},f}(h_{S}\} \subseteq M $$
5. Далее представим $M$ как объединение всех плохих подмножеств:
$$M = \Union_{}$$

*Следствия:*
1. [[3. PAC обучение]] Для известного пространства гипотез можно определить сверху минимальное количество обучающих примеров $m$:
$$ m \leq \frac{log |H|/\delta}{\epsilon}, $$
где $\delta$ - степень уверенности в результате обучения, т.е. вероятность правильного ответа $\mathbb{P}\left[ \mathbb{L}_{\mathbb{D},f}(h_{S}) > \epsilon \right]$ (не хуже $\epsilon$)

---

[^1]: Например можно дискретизировать все непрерывные переменные из $X$ в задаче и ограничить минимальное и максимальное их значение. Можно, например, ограничиться вычислительной мощностью или количеством строк кода и т.п. В целом, однако, на практике все алгоритмы работают, конечно с конечным $H$.