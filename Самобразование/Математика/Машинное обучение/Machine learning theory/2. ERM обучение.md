#ml/erm

---

* Особенность метрики $\mathbb{L}_{\mathbb{D}, f}(h)$ #ml/general_loss в том, что она опирается на неизвестное распределение $\mathbb{D}: X \times Y \rightarrow [0, 1]$. Все что доступно - это выборка $S = \{(x, y): x \sim \mathbb{D},\ y = f(x)\}$ или, в случае если предположение о существовании $f: X \rightarrow Y$ не используется, то $S \subset (X \times Y) \sim \mathbb{D}$. 
* Здесь и далее мы будем опираться на существование функции $f$  и, более того, мы предполагаем важное допущение #ml/terms/reliability_assumption:
$$f \in H$$
	то есть то, что данная функция лежит в пространстве гипотез из которых выбирается гипотеза для классификации (регрессии).
* Таким образом, мы можем определить только *эмпирический риск* (*empirical risk* #ml/tems/empirical_risk):
$$ \mathbb{L}_{S}(h) = \frac{|\{x : x \in S, h(x) \neq f(x)\}|}{|S|} $$
* Алгоритм обучения, который опирается на минимизацию эмпирического риска называется ERM алгоритмом (*empirical risk minimization*):
$$h_S = \min\limits_{h \in H} \mathbb{L}_{S}(h)$$
* Если не ограничивать $H$ никак, то при $|H| = \infty$ алгоритм машинного обучения всегда будет переобучаться, то есть всегда будет возможность найти такое $h$, что $\mathbb{L_{S}(h)} = 0$ и при этом $\mathbb{L_{\mathbb{D},d}}(h) > 0$. Простой пример:
$$h(x) = \begin{cases}
	1,\ if\ x \in S \\
	0,\ otherwise
\end{cases}$$
* Однако есть теорема, которая говорит, что если #ml/terms/reliability_assumption соблюдается и $|H| < \infty$ [^1], то существует алгоритм $ERM_H$, который сходится к решению с любой наперед заданной точностью в $\mathbb{L}_{\mathbb{D},f}(h)$

--

**Теорема:** #ml/erm_h/theorem
> Если:
>	а) зафиксировано #ml/terms/reliability_assumption , то есть существует в пространстве гипотез функция $f$, которая всегда дает верный ответ ($\mathbb{L}_{\mathbb{D},f}$(f) = 0);
>	б) пространство гипотез конечно $|H| < \infty$;
>	в) строки в выборке $S$ представляют собой совокупность независимых друг от друга и одинаково распределенных случайных векторов ( #ml/terms/iid )
>То:
>	

*Доказательство:*

---

[^1]: Например можно дискретизировать все непрерывные переменные из $X$ в задаче и ограничить минимальное и максимальное их значение. Можно, например, ограничиться вычислительной мощностью или количеством строк кода и т.п. В целом, однако, на практике все алгоритмы работают, конечно с конечным $H$.